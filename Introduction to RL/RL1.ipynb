{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAiHkLheiMCW"
   },
   "source": [
    "Please note this lab requires gym package <= 0.20. You can directly use pip install gym == 0.2 if you haven't installed gym package before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI3IxXQWiMCa"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1677881800462,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "ydEXvl5xiMCa",
    "nbgrader": {
     "checksum": "4eed621d3748a44866956caa0de5247b",
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3177,
     "status": "ok",
     "timestamp": 1677881803635,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "ePrbGJ_LiMCb"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "CeQgtvxZiMCb",
    "nbgrader": {
     "checksum": "bab7b3976d6730a0739fd462766b1d42",
     "grade": false,
     "grade_id": "cell-9ebb0d5b306dbdea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "_YffEMUEiMCb",
    "nbgrader": {
     "checksum": "8d010aef9b5b288e694006a2aefe67e0",
     "grade": false,
     "grade_id": "cell-1078e8f0b90517ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this exercise we will evaluate a policy, e.g. find the value function for a policy. The problem we consider is the gridworld from Example 4.1 in Sutton's Reinforcement Learning book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 645,
     "status": "error",
     "timestamp": 1677881804277,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "tZI_etwTiMCc",
    "nbgrader": {
     "checksum": "990081b68602e7e0c46f2edeab0fcb53",
     "grade": false,
     "grade_id": "cell-de586c5ac92d8d74",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "f62734b1-89a9-41d0-d7a7-8e8796250589"
   },
   "outputs": [],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1677881804278,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "YdXfD8UbiMCc",
    "nbgrader": {
     "checksum": "814f4db75653991276d29ebff9d6ae37",
     "grade": false,
     "grade_id": "cell-b3a84dfb0e66a0c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "_YnzWb0fiMCd",
    "nbgrader": {
     "checksum": "fe850a3b9a1be42ae79b895d206ac3b6",
     "grade": false,
     "grade_id": "cell-b2162d776f0c2014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](http://www.incompleteideas.net/book/RLbook2020.pdf) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1677881804278,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "qev_qa5UiMCd"
   },
   "outputs": [],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "aborted",
     "timestamp": 1677881804278,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "-DXdKAF0iMCe",
    "nbgrader": {
     "checksum": "d2d2b829d45d264cf8a6194dc8ccc132",
     "grade": false,
     "grade_id": "cell-209a484040bd874f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v = np.zeros(env.nS)\n",
    "        for s in range(env.nS):\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][action]:\n",
    "#                     print(prob, next_state, reward, done)\n",
    "                    v[s]+= action_prob*prob*(reward+discount_factor*V[next_state])\n",
    "        \n",
    "            delta = max(delta, abs(v[s]-V[s]))\n",
    "            V[s] = v[s]\n",
    "        if delta < theta:\n",
    "            break\n",
    "        \n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1677881804279,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "15E_aajHiMCe"
   },
   "outputs": [],
   "source": [
    "#Your code here: Please initialize a random policy that would move to each direction with 1/4 probability\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "#End of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_eval(random_policy, env)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1677881804279,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "A5zjfTrRiMCf"
   },
   "outputs": [],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4957,
     "status": "aborted",
     "timestamp": 1677881804280,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "oGcnKqnAiMCf",
    "nbgrader": {
     "checksum": "5d879d65fc89af254883e1b68234e76e",
     "grade": true,
     "grade_id": "cell-b5c9d69b1731aff5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "8drW8xMPiMCf",
    "nbgrader": {
     "checksum": "863ed58baecbbb4923162f40084e870d",
     "grade": false,
     "grade_id": "cell-b680e98c9ff204b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Policy Iteration\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4955,
     "status": "aborted",
     "timestamp": 1677881804280,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "Ku4B6UXuiMCg"
   },
   "outputs": [],
   "source": [
    "# calculate for each action in A, for state s\n",
    "def backup(V, s, env, policy, discount_factor=1.0):\n",
    "    V_a = np.zeros(env.nA)\n",
    "    \n",
    "    #NOTE: no expectation over actions, calculate for all actions in your Action space\n",
    "        \n",
    "    for action in range(env.nA) :\n",
    "        policy[s,action] = 0\n",
    "        for prob, next_state, reward, done in env.P[s][action]:\n",
    "             V_a[action]+= prob*(reward+discount_factor*V[next_state])\n",
    "    \n",
    "    return V_a, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 4955,
     "status": "aborted",
     "timestamp": 1677881804281,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "ncyUgWUIiMCg",
    "nbgrader": {
     "checksum": "cfa494b2b437f9007f6b29b1ed5e0f78",
     "grade": false,
     "grade_id": "cell-383c54749617512c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "#     print(np.arange(env.nA), policy[0])\n",
    "#     np.random.choice(int(np.arange(env.nA)), policy[0])\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        V = policy_eval(policy, env, discount_factor)\n",
    "        \n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            #get an old action             \n",
    "            #old_action = np.random.choice(np.arange(env.nA), 1, list(policy[s]))\n",
    "            old_action = np.argmax(policy[s])\n",
    "            \n",
    "            #calculate V(s) for each s: by backup diagram (one-step lookahead)\n",
    "            action_values, policy = backup(V, s, env, policy)\n",
    "            #print(policy)\n",
    "            \n",
    "            a_max_i = np.argwhere(action_values == np.amax(action_values))\n",
    "            policy[s,a_max_i.flatten()] = 1\n",
    "            policy[s,:] = policy[s,:]/np.sum([policy[s,:]])\n",
    "             \n",
    "            if np.argmax(policy[s,:])!=old_action:\n",
    "                policy_stable = False\n",
    "       \n",
    "        \n",
    "        if policy_stable:\n",
    "            break \n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 4949,
     "status": "aborted",
     "timestamp": 1677881804281,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "Y0b4ZCIyiMCg",
    "nbgrader": {
     "checksum": "c4ab9c8d01a5902c276a3fbfbcc89e01",
     "grade": true,
     "grade_id": "cell-8c62e92d1f34720b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5295,
     "status": "aborted",
     "timestamp": 1677881804629,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "36S8hWkEiMCg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 5295,
     "status": "aborted",
     "timestamp": 1677881804630,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "IvTWoKWOiMCh",
    "nbgrader": {
     "checksum": "eee712b931eb830cb89792ef30675558",
     "grade": true,
     "grade_id": "cell-695dc14dbc6a8f95",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "JmFr_GrEiMCh",
    "nbgrader": {
     "checksum": "c926edcbca292c79812c5b27eab63108",
     "grade": false,
     "grade_id": "cell-ceb755afcff43612",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Value Iteration\n",
    "Now implement the value iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 5294,
     "status": "aborted",
     "timestamp": 1677881804630,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "PFiW3kpViMCh",
    "nbgrader": {
     "checksum": "87f515e22f7ad0ea461271479dff3f5e",
     "grade": false,
     "grade_id": "cell-574fc5f6932fa4cc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    # Implement!\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Get action_values and best_val\n",
    "            # YOUR CODE HERE\n",
    "            action_values,_ = backup(V, s, env, policy) \n",
    "            best_val = np.max(action_values)\n",
    "            # End of code\n",
    "            \n",
    "            delta = max(delta, abs(best_val-V[s]))\n",
    "            V[s] = best_val\n",
    "            \n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    #calculate optimal policy\n",
    "    for s in range(env.nS):\n",
    "        action_values, policy = backup(V, s, env, policy)\n",
    "        \n",
    "        a_max_i = np.argwhere(action_values == np.amax(action_values))\n",
    "        policy[s,a_max_i.flatten()] = 1\n",
    "        policy[s,:] = policy[s,:]/np.sum([policy[s,:]])\n",
    "      \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 5289,
     "status": "aborted",
     "timestamp": 1677881804630,
     "user": {
      "displayName": "Shunan Jiang",
      "userId": "16069252254404118689"
     },
     "user_tz": 480
    },
    "id": "fU4UjfMMiMCh",
    "nbgrader": {
     "checksum": "d103427f5b98a8957ad486243f98e64c",
     "grade": true,
     "grade_id": "cell-b82ed3adfeecc757",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Oh let's test again\n",
    "# Let's see what it does\n",
    "policy, v = value_iteration(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "i6O-_IZIiMCh",
    "nbgrader": {
     "checksum": "3add7d8b6101d0e3b6250b6bb064566c",
     "grade": false,
     "grade_id": "cell-ded21ac846e244a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What is the difference between value iteration and policy iteration? Which algorithm is most efficient (e.g. needs to perform the least *backup* operations)? Please answer *concisely* in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "mAPCP4aBiMCi",
    "nbgrader": {
     "checksum": "078f713af4c6bf3af8fb31b8da772758",
     "grade": true,
     "grade_id": "cell-940a8d8e21f18f69",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two popular algorithms for solving Markov Decision Processes (MDPs) and obtaining the optimal policy are value iteration and policy iteration. The key difference between them lies in how they update the policy and the value function.\n",
    "\n",
    "With value iteration, an initial value function is used and iteratively updated until it converges to the optimal value function. At each iteration, the value of each state-action pair is computed and used to update the value function. The optimal policy is then computed based on the updated value function.\n",
    "\n",
    "Policy iteration, on the other hand, starts with an initial policy and gradually improves it until it converges to the optimal policy. At each iteration, the policy is evaluated to obtain the value function, and the policy is then updated based on the value function.\n",
    "\n",
    "Although policy iteration converges faster than value iteration due to updating both the policy and value function, it requires more computation per iteration, which means that the total number of operations may be higher for policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a05f4eab8a48e0fb699a9c4d2ff5136ad6fe08181e3a510e3ac6da38970e2134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
